# Ollama Configuration (Default)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b-instruct

# vLLM Configuration (Alternative for Production)
VLLM_BASE_URL=http://localhost:8000/v1
VLLM_MODEL=meta-llama/Llama-3.1-8B-Instruct

# LLM Provider Selection (ollama, vllm, or gemini)
LLM_PROVIDER=ollama

# Search Tools API Keys
PERPLEXITY_API_KEY=your_perplexity_api_key_here
TAVILY_API_KEY=your_tavily_api_key_here

# Optional: OpenAI API Key for fallback or comparison
# OPENAI_API_KEY=your_openai_api_key_here

# CrewAI Configuration
CREWAI_TELEMETRY_OPT_OUT=true

# Application Settings
DEBUG=true
LOG_LEVEL=INFO