# Ollama Configuration (Default)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b-instruct

# vLLM Configuration (Alternative for Production)
VLLM_BASE_URL=http://localhost:8000/v1
VLLM_MODEL=meta-llama/Llama-3.1-8B-Instruct

# Gemini Configuration (Fallback)
GEMINI_MODEL=gemini-flash-latest
GEMINI_API_KEY=<your_gemini_api_key>

# OpenAI Configuration (Direct API)
# Uncomment and set provider to use direct OpenAI API
# To use GPT-5 via OpenAI, set LLM_PROVIDER=openai and OPENAI_MODEL=gpt-5
# OPENAI_BASE_URL=https://api.openai.com/v1
# OPENAI_MODEL=gpt-5
# OPENAI_API_KEY=<your_openai_api_key>

# LLM Provider Selection (ollama, vllm, local, openai, or gemini)
LLM_PROVIDER=local

# Search Tools API Keys
PERPLEXITY_API_KEY=<your_perplexity_api_key>
TAVILY_API_KEY=<your_tavily_api_key>

# Optional: API key for local OpenAI-compatible servers
# Use a dummy key if your local server requires one
OPENAI_API_KEY=sk-no-key-required

# CrewAI Configuration
CREWAI_TELEMETRY_OPT_OUT=true

# Application Settings
DEBUG=true
LOG_LEVEL=INFO

# Local OpenAI-compatible Configuration (for any local server with OpenAI-like API)
LOCAL_BASE_URL=http://localhost:8000/v1
LOCAL_MODEL=local-model
