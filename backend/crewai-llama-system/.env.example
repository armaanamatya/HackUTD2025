# Ollama Configuration (Default)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b-instruct

# vLLM Configuration (Alternative for Production)
VLLM_BASE_URL=http://localhost:8000/v1
VLLM_MODEL=meta-llama/Llama-3.1-8B-Instruct

# Gemini Configuration (Fallback)
GEMINI_MODEL=gemini-2.5-flash-latest
GEMINI_API_KEY=<your_gemini_api_key>

# LLM Provider Selection (ollama, vllm, or gemini)
LLM_PROVIDER=gemini

# Search Tools API Keys
PERPLEXITY_API_KEY=<your_perplexity_api_key>
TAVILY_API_KEY=<your_tavily_api_key>

# Optional: OpenAI API Key for fallback or comparison
# OPENAI_API_KEY=your_openai_api_key_here

# CrewAI Configuration
CREWAI_TELEMETRY_OPT_OUT=true

# Application Settings
DEBUG=true
LOG_LEVEL=INFO

# Local OpenAI-compatible Configuration (for any local server with OpenAI-like API)
LOCAL_BASE_URL=http://localhost:8000/v1
LOCAL_MODEL=local-model
